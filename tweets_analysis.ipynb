{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b50e60a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets about Apple and Google Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439856a",
   "metadata": {},
   "source": [
    "### Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d9734",
   "metadata": {},
   "source": [
    "\n",
    "This notebook builds an NLP model to classify sentiment in tweets directed at Apple and Google products.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81606938",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b709f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\A808865\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\A808865\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\A808865\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\A808865\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\A808865\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# pandas: for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# re: Python's built-in library for regular expressions (used for text cleaning)\n",
    "import re\n",
    "\n",
    "# nltk: Natural Language Toolkit, useful for tokenization, stopword removal, and lemmatization\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")        # tokenizer model\n",
    "\n",
    "nltk.download(\"punkt_tab\")    # sentence boundary detection\n",
    "\n",
    "nltk.download(\"wordnet\")      # lexical database for lemmatization \n",
    "\n",
    "nltk.download(\"omw-1.4\")      # WordNet data for multiple languages\n",
    "\n",
    "nltk.download(\"stopwords\")    # common words to filter out (e.g., \"the\", \"is\")\n",
    "\n",
    "# Import stopwords list from nltk (words to ignore during analysis)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import tokenizer to split text into individual words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import lemmatizer to reduce words to their base form (e.g., \"running\" → \"run\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# TfidfVectorizer: convert text data into numerical features using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# train_test_split: split data into training and testing sets for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df11e1",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49212dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the CSV file with correct encoding\n",
    "df = pd.read_csv('Data\\judge-1377884607_tweet_product_company.csv', encoding='Latin-1')\n",
    "\n",
    "# Displaying the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53891c85",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "- In order to better understand the dataset and prepare it for sentiment analysis, we will focus on the following checks:\n",
    "    - Preview the data: Inspect the first few rows to quickly grasp the dataset’s structure.\n",
    "    - Detect any missing values in the data that could introduce bias or cause issues during preprocessing and modeling.\n",
    "    - Identify and remove duplicate tweets to prevent overrepresentation of certain entries, which could distort the sentiment model.\n",
    "    - Review the balance of sentiment categories, since skewed classes may result in models that favor majority classes and perform poorly on minority ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efb0a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the data set\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79733fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4aca66",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e93d396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any rows where the tweet text is missing, because a tweet with no text is useless for sentiment analysis.\n",
    "df.dropna(subset=[\"tweet_text\"], inplace=True)\n",
    "\n",
    "# For the column emotion_in_tweet_is_directed_at, instead of dropping missing values, it replaces them with \"Unknown\". This way you don’t lose the tweet itself — you just acknowledge that the target of the emotion is not specified.\n",
    "df.fillna({'emotion_in_tweet_is_directed_at': 'Unknown'}, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14d364c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                            0\n",
       "emotion_in_tweet_is_directed_at                       0\n",
       "is_there_an_emotion_directed_at_a_brand_or_product    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7d354",
   "metadata": {},
   "source": [
    "### Step 3: Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44dc9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"tweet_text\"].apply(clean_tweet_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ff655",
   "metadata": {},
   "source": [
    "### Step 4: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6b081b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"clean_text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d062af6",
   "metadata": {},
   "source": [
    "### Step 5: Stopward Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2fcf6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d81351",
   "metadata": {},
   "source": [
    "### Step 6: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2e78897",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe29712",
   "metadata": {},
   "source": [
    "### Step 7: Join Tokens Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b74adade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f2def",
   "metadata": {},
   "source": [
    "### Step 8: Vectorization (TF-IDF Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9670b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF - IDF shape (9092, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"processed_text\"])\n",
    "\n",
    "print('TF - IDF shape', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6e3a1",
   "metadata": {},
   "source": [
    "### Step 9: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7b34a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (7273, 5000)\n",
      "Test size: (1819, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"is_there_an_emotion_directed_at_a_brand_or_product\"], test_size=0.2, random_state=42)\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f611b",
   "metadata": {},
   "source": [
    "## Refactoring the steps above into a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "06970c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tweet_text  \\\n",
      "0   I G iPhone After hr tweeting dead I need upgra...   \n",
      "1   Know Awesome iPadiPhone app youll likely appre...   \n",
      "2                             Can wait also They sale   \n",
      "3    I hope year festival isnt crashy year iPhone app   \n",
      "4   great stuff Fri Marissa Mayer Google Tim OReil...   \n",
      "5   New iPad Apps For And Communication Are Showca...   \n",
      "7   starting around corner hop skip jump good time...   \n",
      "8     Beautifully smart simple idea RT wrote iPad app   \n",
      "9   Counting day plus strong Canadian dollar mean ...   \n",
      "10  Excited meet I show Sprint Galaxy S still runn...   \n",
      "\n",
      "   emotion_in_tweet_is_directed_at  \\\n",
      "0                           iPhone   \n",
      "1               iPad or iPhone App   \n",
      "2                             iPad   \n",
      "3               iPad or iPhone App   \n",
      "4                           Google   \n",
      "5                          Unknown   \n",
      "7                          Android   \n",
      "8               iPad or iPhone App   \n",
      "9                            Apple   \n",
      "10                         Android   \n",
      "\n",
      "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
      "0                                    Negative emotion  \n",
      "1                                    Positive emotion  \n",
      "2                                    Positive emotion  \n",
      "3                                    Negative emotion  \n",
      "4                                    Positive emotion  \n",
      "5                  No emotion toward brand or product  \n",
      "7                                    Positive emotion  \n",
      "8                                    Positive emotion  \n",
      "9                                    Positive emotion  \n",
      "10                                   Positive emotion  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Download NLTK resources (leave commented if already downloaded)\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# Custom Preprocessor\n",
    "# -------------------------------\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_column):\n",
    "        self.text_column = text_column\n",
    "\n",
    "    def clean_text(self,text):\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text) # remove urls\n",
    "        text = re.sub(r\"@\\w+\", \"\", text) # remove mentions\n",
    "        text = re.sub(r\"#\\w+\", \"\", text) # remove hashtags\n",
    "        text = re.sub(r\"[^A-Za-z\\s]\", \"\", text) # remove special characters\n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_lemmatize(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_filled = X.copy()\n",
    "        \n",
    "        # Fill missing values in the second column\n",
    "        second_col = X_filled.columns[1]\n",
    "        X_filled[second_col] = X_filled[second_col].fillna(\"Unknown\")\n",
    "        \n",
    "        # Process the text column\n",
    "        X_filled[self.text_column] = X_filled[self.text_column].apply(\n",
    "            lambda t: self.tokenize_lemmatize(self.clean_text(t))\n",
    "        )\n",
    "        \n",
    "        return X_filled  # return as DataFrame\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "# -------------------------------\n",
    "# Load Dataset\n",
    "# -------------------------------\n",
    "    \n",
    "df = pd.read_csv(\"Data\\judge-1377884607_tweet_product_company.csv\", encoding='Latin-1')\n",
    "\n",
    "df.dropna(subset=[\"tweet_text\"], inplace=True)\n",
    "\n",
    "X = df[\"tweet_text\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Build Preprocessing Pipeline\n",
    "# -------------------------------\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"text_preprocessor\", TextPreprocessor(text_column=\"tweet_text\"))\n",
    "])\n",
    "\n",
    "# Apply Pipeline\n",
    "df_preprocessed = preprocessing_pipeline.fit_transform(df)\n",
    "\n",
    "# view processed tweets\n",
    "print(df_preprocessed.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
